{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xbb Tagger Calibration Package Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### What is an Xbb tagger?\n",
    "\n",
    "Think of this tagger in this way. After a series of collisions which happened in LHC, we collect a bunch of data and reconstruct them into many jets (Assume you are already familier with the concept of jet after the jet tutorial you went through). However, having these jets in hand, we are particularly interested in bb dijets. Xbb tagger is a tool which selects out all the bb jets from this vast ocean of jets, like a robot putting a \"this is a bb jet\" tag on all the bb jets, just as its name implies. \n",
    "\n",
    "\"So why do we care about bb jets?\", you may ask. This is because by prediction, some new unknown heavy particle (which we are desperately looking for) has Higgs boson included in its decay chains, and, by the previous study, Higgs would again decays into b and b-bar. Hence, in order to spot out the new heavy particle from the up stream, we need first isolate out all the bb jets at the down stream, this is why this tagger cares exclusively about bb jets. However, since H->bb decay is not easy to find, so we use g->bb decays instead to examine this tagger (On principle, Xbb tagger should be valid for identifing what ever particle decaying into bb, this is why we call it **X**bb, **X** as a sence of a variable). To identify g->bb events, we use the fact that most of the time in an g->bb decay, one of the b jets would include a muon. So as you go through the package, you will see some names like **Muon Inclusive** or **Muon Filtered**, and here is why. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/gbb_decay_diagram.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "### What is scale factor and how do we calculate it?\n",
    "\n",
    "From the jet tutorial, you learned about how we use Monte Carlo (MC for short) simulation in high energy physics. The scale factor (SF for short) is baiscally a fraction comparing our tagger's efficiencies when applying on MC simulation and true measured data sample. A quick graphical definition is shown below:\n",
    "\n",
    "\n",
    "<img src=\"img/SF_def.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Where $\\epsilon$ represents tagger efficiency and its subscripts denote two types of efficiency mentioned above. $N _{BB, total}$ coresponds to the number of bb jet before tagging, and $N _{BB, double-b-tagged}$ is the number of bb jet after tagging. Calculating the scale factor $\\kappa _{SF}$ is by just take the quotient of two $\\epsilon$. \n",
    "\n",
    "So we now know the definition of the SF, but how do we actually calculate it from all the sample sets we have in hand? Here is a breif discription of the percedure:\n",
    "\n",
    "**1. Get the number of BB events in MC to obtain $N^{MC}_{BB, total}$**\n",
    "\n",
    "From our MC simulation, the total number of BB events is already known (dark blue area). (Notice that we have classify the events into different bins according to their $s_{d0}$ to get a more percise result)\n",
    "\n",
    "<img src=\"img/MC_pretag.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "**2. Apply double b tagging on MC to get $N^{MC}_{BB, double-b-tagged}$**\n",
    "\n",
    "After applying the tagger, we could see that most of the events have been removed (LL, CL, CC, BL) except BB events. The number of BB events is still known (area of dark blue).\n",
    "\n",
    "<img src=\"img/MC_posttag.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "At this point, we could already calculate $\\epsilon_{MC}$ by taking the ration of dark blue area from two graphs above.\n",
    "\n",
    "**3. Do fitting on (1.) to get $N^{data}_{BB, total}$**\n",
    "\n",
    "From the above graphs, notice the black dots denoted *Data*, those are the total number of events we **measured** at LHC. In each bin, we don't know how many BB events we have, all we know is **the sum** of all events (i.e. LL, CL, CC, BL, BB). Here, we will make an important assumption: **\"After applying fitting on MC, the proportion of BB events is the same as which in measured data\"**. So, by scaling (or, in other words, fitting) the MC event number (the number including all events, not just BB) in each bin to match the data event number, we could obtain the BB events number in data. \n",
    "\n",
    "<img src=\"img/data_total.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "**4. Do fitting on (2.) to get $N^{data}_{BB, double-b-tagged}$**\n",
    "\n",
    "<img src=\"img/data_bb.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "By taking the ratio of dark bule area again for the fitted graphs, you obtain $\\epsilon_{data}$.\n",
    "\n",
    "**5. Calculate $\\kappa_{SF} = \\frac{\\epsilon_{data}}{\\epsilon_{MC}}$**\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"img/efficiencies.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "The above graphs are $\\epsilon_{MC}$ and $\\epsilon_{data}$, subdivied by the transverse momentum of each jets (i.e. non-$\\mu$ and $\\mu$).\n",
    "Divide the number in the right graph by the number in the left (reginal wise), you obtain the SF as below.\n",
    "\n",
    "<img src=\"img/SF.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "\n",
    "So what's the use of this scale factor? One use of it is to calculate $\\epsilon_{data}$, this is easy since $\\epsilon_{MC}$ and $\\kappa_{SF}$ are all known numbers (of course, after we do the scale factor calculation). Another use of the scale factor is to check the reliability of our MC simulation. Logically speaking, if our MC modle simulates real events fairly truthfully, $\\epsilon_{MC}$ and $\\epsilon_{data}$ should be about the same, that is, the scale factor $\\kappa_{SF} = \\frac{\\epsilon_{data}}{\\epsilon_{MC}}$ should be close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "### First time set up\n",
    "\n",
    "#### Applying for a NERSC account\n",
    "First of all, you will need to apply for a **NERSC** account. Please contact your supervisor to do so. NERSC is a scientific computing facility for the Office of Science in the U.S. Department of Energy. Through your account, you could log into the super computer at NERSC from your local machine and do all your work over there. Beaware that NERSC has several nodes avalible, most of our group members use **Cori**. \n",
    "\n",
    "After you have your NERSC account, try logging in using:\n",
    "```console\n",
    "$ ssh -Y **myusername**@</span><span style=\"color:red\">cori.nersc.gov\n",
    "```\n",
    "\n",
    "where **myusuername** should be replaced by your NERSC log in ID.  \n",
    "Then you will be prompted to enter your NERSC password and one time password. Note: **DO NOT** put any space between these two passwords when entering to your terminal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up NoMachine\n",
    "NoMachine provides you a virtual desktop to work with, so when you disconnecte your laptop from the internet, you won't be logged out by Cori, all your previous work will still be there. Moreover, you will only need to key in your password when first connecting to your virtual desktop everyday. If you have setup Cori successfully, you know by now how cumbersome it is to login everytime you open a new tab.\n",
    "\n",
    "Here is the link to a NERSC page explaning on how to setup NoMachine: \n",
    "<a href=\"https://docs.nersc.gov/connect/nx/\" target=\"_blank\">NoMachine setup</a>.\n",
    "\n",
    "You could read through it and follow the instruction there if you wish, but here is the steps on how to setup NoMachine:\n",
    "\n",
    "**1**. First you need to instal the NoMachine Client on your local machine. Go to the according page base on your operating system: <a href=\"https://www.nomachine.com/download/download&id=15\" target=\"blank\">Mac</a>, <a href=\"https://www.nomachine.com/download/linux&id=4\" target=\"blank\">Linux</a>, <a href=\"https://www.nomachine.com/download/download&id=16\" target=\"blank\">Windows</a>.\n",
    "\n",
    "\n",
    "**2**. At your local machine, download the bash client sshproxy.sh from **<span>myusername</span>**@cori.nersc.gov:/project/projectdirs/mfa/NERSC-MFA/sshproxy.sh:  \n",
    "```console\n",
    "$ cd \n",
    "$ scp **myusername**@dtn01.nersc.gov:/project/projectdirs/mfa/NERSC-MFA/sshproxy.sh .\n",
    "```\n",
    "\n",
    "\n",
    "**3**. Now run the sshproxy.sh  \n",
    "```console\n",
    "$ ./sshproxy.sh -u **myusername**\n",
    "```\n",
    "\n",
    "Then you will be prompt to enter your password + OTP. If everything has been done correctly, you will see the following.  \n",
    "<span style=\"color:red\">Successfully obtained ssh key /Users/wyang/.ssh/nersc  \n",
    "Key /Users/**myusername**/.ssh/nersc is valid: from 2018-08-30T12:24:00 to 2018-08-31T12:25:52</span>\n",
    "\n",
    "Now in your <span style=\"color:red\">~/.ssh</span> folder, you should see three files got generated: <span style=\"color:red\">nersc</span>, <span style=\"color:red\">nersc-cert.pub</span>, and <span style=\"color:red\">nersc.pub</span>.\n",
    "\n",
    "\n",
    "**4**. Open the NoMachine client and click the \"New\" box in the upper right corner of the menu.\n",
    "\n",
    "\n",
    "**5**. Select Protocol to be \"SSH\" and click Continue.\n",
    "\n",
    "\n",
    "**6**. Type in \"nxcloud01.nersc.gov\" for Host (leave the port set to 22) and click Continue>\n",
    "\n",
    "\n",
    "**7**. Choose \"Private key\" and click Continue.\n",
    "\n",
    "\n",
    "**8**. Fill in the path of the key you generated in step 3, <span style=\"color:red\">~/.ssh/nersc </span>, and click Continue.\n",
    "\n",
    "\n",
    "**9**. Select \"Don't use a proxy\" and click Continune.\n",
    "\n",
    "\n",
    "**10**. Type in the name you like for your connection. An easy understandable name would be something like \"Connection to cori.nersc.gov\". After naming your connection, click Done. Now you should see the new connection you just created on your NoMachine interface.  \n",
    "\n",
    "\n",
    "**11**. Exit and close NoMachine completely (i.e. not running). Open the file at <span style=\"color:red\">~/.nx/config/player.cfg</span> and change the following key from <span style=\"color:red\">library</span> to <span style=\"color:red\">native</span>:  \n",
    "\n",
    "<span style=\"color:red\"><option key=\"SSH client mode\" value=\"library\" / > </span>\n",
    "\n",
    "\n",
    "**12**. Fianlly, click on the NoMachine icon and connect to Cori by clicking the connection image you created in step 10. Enter your NERSC passwaord + OTP if prompted, then click on \"Create a new virtual desktop\". Now you have succefully created your virtual desktop, we will then do everything followed in this tutorial here. **NOTE: the ssh key you obtained using sshproxy only has a life time of 24hr.** So, if you get an error of \"authentication fail\" while connecting to Cori through NoMachine, which means your key has expired. Run step 3 again then you could be able to connect to Cori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enviroment setup  \n",
    "Open your virtual desktop and go to the terminal, login to Cori with  \n",
    "```bash\n",
    "$ ssh -Y myysername@cori.nersc.gov\n",
    "```\n",
    "\n",
    "In order to use Cori on most of the ATLAS sites, you will need to add a missing library located at <span style=\"color:red\">/global/project/projectdirs/atlas/scripts/extra_libs_180822</span> to <span style=\"color:red\">LD_LIBRARY_PATH</span>. To do this, you need to madify your .bashrc. The calibration package also requires you to have a few additional lines in .bashrc. Open .bash `$ vi ~/.bashrc` and add the following lines at the end.  \n",
    "<span style=\"color:red\"> export LD_LIBRARY_PATH=/global/project/projectdirs/atlas/scripts/extra_libs_180822:\\$LD_LIBRARY_PATH</span>   \n",
    "<span style=\"color:red\">export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase  \n",
    "    alias setupATLAS='source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh' </span>\n",
    "\n",
    "Now, we will have to set the permission correctly. Run the following commands:  \n",
    "```bash\n",
    "$ rm -rf ~/.alrb\n",
    "$ chmod g+s $HOME \n",
    "$ setfacl -d -m u:nobody:rx $HOME \n",
    "$ setfacl -m u:nobody:rx .\n",
    "```\n",
    " \n",
    "If you run into errors when setting up a container, you can try this to see if the problem is solved:  \n",
    "```bash\n",
    "$ setfacl -m u:nobody:x ~/  \n",
    "$ setfacl -m u:nobody:x ~/.alrb/   \n",
    "$ setfacl -m u:nobody:x ~/.alrb/container  \n",
    "$ setfacl -m u:nobody:x ~/.alrb/container/shifter\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Obtaining the gbb calibration package\n",
    "Clone the gbb calibration package at https://gitlab.cern.ch/atlas-boosted-hbb/gbbCalibPackage.git  \n",
    "```bash\n",
    "$ git clone ssh://git@gitlab.cern.ch:7999/aemerman/gbbCalibPackage.git\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everytime setup\n",
    "\n",
    "**NOTE**: You need to do the following setup everytime you log in to Cori. Of course, if you never log off from Cori on your virtual desktop, you will only need to do this section once.\n",
    "\n",
    "First, setup the ATLAS Local Root Base by sourcing the following script:\n",
    "```bash\n",
    "$ cd ~/\n",
    "$ source /global/project/projectdirs/atlas/scripts/setupATLAS.sh\n",
    "```\n",
    "Then, set up a container for running the calibration package:\n",
    "```bash\n",
    "$ setupATLAS -c zlmarshall/atlas-grid-slc6:20190711+batch\n",
    "```\n",
    "*Optional: It would be useful to write the command above into a bash script, so you don't need to type in the long source file and container names eveytime you log in.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First time using the package\n",
    "\n",
    "**NOTE:** This section is only need to be done **once** after you clone the package. Do not do it everytime you log in. \n",
    "\n",
    "We need first to compile the package using cmake. At the directory you downloded the package, run the following commands:\n",
    "```bash\n",
    "$ cd gbbCalibPackage/build/\n",
    "$ cmake ../source\n",
    "$ make\n",
    "```\n",
    "After compilation, source the file you just created at <span style=\"color:red\">build/x86_64-slc6-gcc62-opt/setup.sh</span>\n",
    "```bash\n",
    "$ source ./x86_64-slc6-gcc62-opt/setup.sh\n",
    "```\n",
    "\n",
    "You will only need to source from the <span style=\"color:red\">build</span> directory for the first tme. In the future, you only need to source the <span style=\"coloe:red\">setup.sh</span> in package mian directory by runing `$ source setup.sh`. **Souring the setup.sh file is necessary everytime you restart you work** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "\n",
    "### Package structure\n",
    "\n",
    "In the package diectory, there are three important directories: build, source, and run. Build is only used in the compilation step, but it also contains the setup.sh file which you need to source everytime you come back to your project. Run is the work space provided for you, it should be empty at this point, all the result from running the code should be stored in this directory. Source contains all the codes you need, and also configuration files where you will set the parameters.\n",
    "\n",
    "To test out the code you could use the data sets located at: <span style=\"color:red\">/global/projecta/projectdirs/atlas/ac1028/inputs-08-05</span>  \n",
    "Notice there are two group of data sets, one staring with <span style=\"color:red\">user.aemerman.mc16*</span> and the other one starting with <span style=\"color:red\">user.aemerman.data17*</span>. From the prefix you might be able to guess that the former is the measured data saet and the later is the MC simulation. Also note that for MC data sets, there are some named <span style=\"color:red\">user.aemerman.mc16_13TeV.361*</span> and some with <span style=\"color:red\">user.aemerman.mc16_13TeV.427*</span>; 361 is the muon inclusive simulation and 427 is the muon filtered, these two channels need to be run seperately sometime when making histogram. \n",
    "\n",
    "At each histogram making step (i.e. run_GbbTupleAna_\\*), there are two ways to run the code: you could either run locally (in the terminal on your virtual desktop) or submit a job using slurm ([section 4. Using slurm](#slurm)). For the first time learning this package, I recommend doing local run, and only pick one data set from data and two channels of MC (so three in total: \\*data17\\*, \\*mc16\\*361,\\* \\*mc16\\*427\\*). After you went throught the whole workflow, you could use slurm to run all data sets provided in the path above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reweighting\n",
    "A lot of times, the event number in MC simulation is way less the the real measured data; hence, we need to scale the MC event number to matche the data event number, and we call this processes reweighting.\n",
    "\n",
    "**discription for how to set the rewight config file**\n",
    "\n",
    "**1. To make the reweighting histogram, do the following:**\n",
    "\n",
    "```bash\n",
    "cd gbbCalibPackage/run\n",
    "mkdir reweight\n",
    "cd reweight\n",
    "run_gbbTupleAna <input> <root_output> <tree_name> ../source/gbbCalibration/data/configs/run_GbbTupleAna_Reweight.cfg\n",
    "```\n",
    "where \n",
    "- input = path to your input txt file (in our case  /global/projecta/projectdirs/atlas/ac1028/inputs-08-05/**file_you_picked_to_run**)\n",
    "- root_output = **name_you_want_for_output**.root (recommend keep it the same as the input txt file name)\n",
    "- tree_name = FlavourTagging_Nominal\n",
    "Run this step once for data, muon filetered, muon inclusve and obtain three root output.\n",
    "\n",
    "**2. hadd you data historgrams into one (data and each MC channel seperately):**\n",
    "If you somehow discided to run more than one data set from any of the three categories, you need to combined them into one historgram.\n",
    "```bash\n",
    "hadd <root_output> <root_input1> <root_input2> ... <root_inputn>\n",
    "```\n",
    "\n",
    "**3. Create a .json file indicates the localtions of \\< root_output \\> :** \n",
    "\n",
    "The file structure is kind of tricky here, arrange your file as the diagram shown, move files and create directoris if needed.\n",
    "\n",
    "<img src=\"img/json_file_structure.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Note: you will only have **THREE** output files.\n",
    "\n",
    "A templete of the jason file\n",
    "\n",
    "```cpp\n",
    "{\n",
    "  \"xsecFile\" : \"data/xsections_r21.txt\",\n",
    "  \"BasePath\" : \"<absolute_path_to_gbbCalibPackage>/run/reweight/\",\n",
    "  \"Data\" : \"<name_of_data_output_root_file>\",\n",
    "  \"MuBase\" : \"MCFil_output/\",\n",
    "  \"IncBase\" : \"MCInc_output/\",\n",
    "  \"Sys\" : [\n",
    "    \"Nominal/\"\n",
    "    ],\n",
    "  \"MuFilteredMC\" : [\n",
    "    \"<name_of_mc_muon_filtered_output_root_file>\"\n",
    "    ],\n",
    "  \"InclusiveMC\" : [\n",
    "    \"<name_of_mc_muon_inclusive_output_root_file>\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "The path to each root output is constructed in the following ways:\n",
    "- absolute path to data_output.root = \"BasePath\" + \"Data\"\n",
    "- absolute path to mcfil_output.root = \"BasePath\" + \"MuBase\" + \"Sys\" + \"MuFilteredMC\"\n",
    "- absolute path to mcinc_output.root = \"BasePath\" + \"IncBase\" + \"Sys\" + \"InclusiveMC\"\n",
    "\n",
    "If you feel like, you could change the file strcture as you like as long as you indicate the path to each root output clearly in .json file according to the path structures above. \n",
    "\n",
    "**4. Run makeReweightingHistos.py to sort through the histograms**\n",
    "In the source direcory, run the makeReweightingHistos.py python script:\n",
    "```bash\n",
    "cd ../source\n",
    "python ./gbbCalibration/python/makeCrossCheckInputsInclusiveLL_AllSys.py <root output> <json input> 0\n",
    "```\n",
    "< root output > = recomand that storing the output in run  \n",
    "< json input > = relative path to the json file you created with respect to source directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Histogram making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculating Scale Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using slurm<a name=\"slurm\"> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
